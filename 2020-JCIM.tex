
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% This is a (brief) model paper using the achemso class
%% The document class accepts keyval options, which should include
%% the target journal and optionally the manuscript type.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[journal=jacsat,manuscript=article]{achemso}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Place any additional packages needed here.  Only include packages
%% which are essential, to avoid problems later.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{chemformula} % Formula subscripts using \ch{}
\usepackage[T1]{fontenc} % Use modern font encodings
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{float}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If issues arise when submitting your manuscript, you may want to
%% un-comment the next line.  This provides information on the
%% version of every file you have used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\listfiles

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Place any additional macros here.  Please use \newcommand* where
%% possible, and avoid layout-changing macros (which are not used
%% when typesetting).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*\mycommand[1]{\texttt{\emph{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Meta-data block
%% ---------------
%% Each author should be given as a separate \author command.
%%
%% Corresponding authors should have an e-mail given after the author
%% name as an \email command. Phone and fax numbers can be given
%% using \phone and \fax, respectively; this information is optional.
%%
%% The affiliation of authors is given after the authors; each
%% \affiliation command applies to all preceding authors not already
%% assigned an affiliation.
%%
%% The affiliation takes an option argument for the short name.  This
%% will typically be something like "University of Somewhere".
%%
%% The \altaffiliation macro should be used for new address, etc.
%% On the other hand, \alsoaffiliation is used on a per author basis
%% when authors are associated with multiple institutions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{Eunji Kim}
\author{Dongseon Lee}
\email{dongseon.lee@samsung.com}
\author{Youngchun Kwon}
\author{Min Sik Park}
\author{Youn-Suk Choi}
\email{ysuk.choi@samsung.com}
\affiliation[SAIT]
{Samsung Advanced Institute of Technology, Samsung Electronics Co., Ltd., 130 Samsung-ro, Yeongtong-gu, Suwon 16678, Republic of Korea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The document title should be given as usual. Some journals require
%% a running title from the author: this should be supplied as an
%% optional argument to \title.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[An \textsf{achemso} demo]
{Valid, Plausible, and Diverse Retrosynthesis using Tied Two-way Transformers with Latent Variables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Some journals require a list of abbreviations or keywords to be
%% supplied. These should be set up here, and will be printed after
%% the title and author information, if needed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\abbreviations{IR,NMR,UV}
%\keywords{Retrosynthesis, , \LaTeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The manuscript does not need to include \maketitle, which is
%% executed automatically.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% The "tocentry" environment can be used to create an entry for the
	%% graphical table of contents. It is given here as some journals
	%% require that it is printed as part of the abstract page. It will
	%% be automatically moved as appropriate.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{tocentry}
		
	\includegraphics{./fig/toc.pdf}

	\end{tocentry}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% The abstract environment will automatically gobble the contents
	%% if an abstract is not used by the target journal.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{abstract}
	Retrosynthesis is an essential task in organic chemistry for identifying the synthesis pathways of newly discovered materials, and with the recent advances in deep learning, there have been growing attempts to solve the retrosynthesis problem through transformer models, which are the state-of-the-art in neural machine translation, by converting the problem into a machine translation problem. However, the pure transformer provides unsatisfactory results that lack grammatical validity, chemical plausibility, and diversity in reactant candidates. In this study, we develop tied two-way transformers with latent modeling to solve those problems using cycle consistency checks, parameter sharing, and multinomial latent variables. Experimental results obtained using public and in-house datasets demonstrate that the proposed model improves the retrosynthesis accuracy, grammatical error, and diversity, and qualitative evaluation results verifies its ability to suggest valid and plausible results.
	\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Start the main part of the manuscript here.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Introduction}
	
	In the field of organic chemistry, retrosynthesis refers to the procedure of identifying a series of precursors that lead to the synthesis of a target product. First used by Robert Robinson in tropinone synthesis \cite{robinson1917lxiii} and formalized by E. J. Corey \cite{corey1969computer,corey1967general,corey1985computer}, retrosynthesis has become one of the core techniques of organic chemistry. Simply, one-step retrosynthesis takes the target product molecule as the input and predicts possible precursor reactants. This process is challenging, as the number of possible chemical decomposition is vast and requires experienced chemists to identify a synthesis pathway for newly discovered materials.
	
	For many years, computer-aided retrosynthesis tools depending on reaction templates have been developed to assist chemists \cite{satoh1995sophia,ihlenfeldt1996computer,todd2005computer,kowalik2012parallel,christ2012mining,cook2012computer,bogevig2015route,szymkuc2016computer}. These methods select and apply the best rule for a given target compound among the known reaction templates. To enhance the accuracy of such methods, Segler and Waller \cite{segler2017neural} used deep neural networks to prioritize the most suitable transformation rules, Coley et al. \cite{coley2017computer} ranked retrosynthetic disconnections based on molecular similarity to precedent reactions, and Dai et al. \cite{dai2019retrosynthesis} modeled the joint distribution of templates and reactants using logic variables. To prepare the reaction templates, users have to explore a myriad of known reactions to prepare a complete set of reaction templates that cover almost all reactions. Templates can be hand-coded based on chemical knowledge \cite{hartenfeller2011collection,szymkuc2016computer} or extracted algorithmically from large reaction databases \cite{law2009route,coley2017prediction}. However, this cumbersome rule identification process does not guarantee high accuracy as the templates cannot cover the entire reaction space and lack adaptability to novel molecules.
	
	Template-free retrosynthesis approaches have emerged in recent years to overcome the limitation of template-based approaches by casting retrosynthesis into a neural machine translation (NMT) problem of translating a source language (product) to a target language (reactants). Molecules are represented in a string format called the simplified molecular-input line-entry system (SMILES) \cite{weininger1988smiles}. Then, the string representation of an input molecule is fed to a fully data-driven neural network model, such as a recurrent neural network (RNN) \cite{nam2016linking, liu2017retrosynthetic} or a transformer \cite{schwaller2019molecular, zheng2019predicting, chen2019learning, karpov2019transformer} originally proposed for NMT, and the string representation of the output molecule is decoded sequentially character by character. These sequence-to-sequence (seq2seq) models have been applied in both forward reaction prediction \cite{nam2016linking, schwaller2019molecular} and retrosynthetic reaction prediction \cite{liu2017retrosynthetic, zheng2019predicting, chen2019learning, karpov2019transformer}.  Whereas RNNs suffer from long-term dependency problems \cite{nam2016linking, liu2017retrosynthetic}, transformers consisting of self-attention and feed-forward modules are free from the recurrent structure and provide better accuracy \cite{schwaller2019molecular, karpov2019transformer, chen2019learning, zheng2019predicting}.
	
	
	\begin{figure*}
		\centering
		\includegraphics[width=\linewidth]{./fig/3problems.pdf}
		\caption{Example of the three types of problems in template-free retorysynthesis}
		\label{fig:problem}
	\end{figure*}
	
	Although template-free models do not require any chemical prior knowledge or reaction templates and exhibit comparable or better accuracy than template-based models, they are likely to provide dissatisfactory results owing to the three types of problems depicted in Figure~\ref{fig:problem}: (i) invalid SMILES: the predicted reactant text representation (candidate 1 in Figure~\ref{fig:problem}) has a grammatical error and cannot be converted into a molecular structure \cite{liu2017retrosynthetic, zheng2019predicting}, (ii) implausibility: the prediction (candidate 2 in the Figure~\ref{fig:problem}) is grammatically valid but cannot synthesize the desired product \cite{liu2017retrosynthetic}, and (iii) lack of diversity: some of the predicted reactant candidates (candidates 3 and 4 in Figure~\ref{fig:problem}) are duplicated, thereby possibly lowering the number of unique suggestions \cite{chen2019learning}. To solve these problems, attempts have been made, such as adding a syntax corrector that can fix a grammatical error in the retrosynthetic prediction \cite{zheng2019predicting} or mixture modeling for diversity \cite{chen2019learning}. However, no studies have tackled all these problems simultaneously.

	In this paper, we propose a retrosynthesis model developed for enhancing the validity, plausibility, and diversity of template-free retrosynthesis results. In particular, we coupled two transformers, one for retrosynthetic (backward) reaction prediction and the other for forward reaction prediction, to ensure that the optimal reactant prediction satisfies cycle consistency, i.e., the forward transformer predicts the original input product, having received the reactant prediction as its input. The two transformers are strongly tied to share most model parameters, making the entire model compact, efficient, and generalizable. To generate diverse reactant candidates, we introduced a multinomial latent variable with a learned prior. To demonstrate the effectiveness of the proposed model, we conducted quantitative and qualitative evaluations using public and in-house datasets. Our model exhibited improved accuracy, grammatical validity, and diversity compared to the baselines. The predicted reactants were chemically plausible, with few grammatical errors. For the products that had multiple synthetic pathways, the proposed model generated a diverse candidate set that covered more pathways than the baselines.
	

	\section{Methods}

	
	\subsection{Problem Description}
	
	
	First, we formally describe the template-free retrosynthesis problem. Given the token sequence of the target product, $x$, the template-free retrosynthesis aims to find the most probable reactant token sequence, $y$, as follows:
	\begin{equation}
	\argmax_{y\in\mathcal{Y}} p(y|x),
	\end{equation}
	where $\mathcal{Y}$ is the set of all possible reactant sequences. The likelihood, $p(y|x)$, is parametrized with seq2seq models such as RNNs \cite{nam2016linking, liu2017retrosynthetic} or transformers \cite{schwaller2019molecular, zheng2019predicting, chen2019learning, karpov2019transformer}. These models consist of an \textit{encoder}, which reads the product sequence, and a \textit{decoder}, which produces a distribution over the reactant sequence given the product autoregressively. The decoding process starts from the starting token `<s>', sequentially generates the reactant sequence by taking the argmax token of the token prediction probabilities, and ends when the argmax token is `</s>', indicating the end of a sequence. To obtain multiple reactant prediction candidates, \textit{beam search} is used to generate and maintain multiple hypotheses at each decoding step.
	
	
	\subsection{Model Overview}
	

	\begin{figure*}
		\centering
		\includegraphics[width=\linewidth]{./fig/overview.pdf}
		\caption{Overview of the proposed model.}
		\label{fig:overview}
	\end{figure*}

	
	Here, we provide an overview of our model for valid, plausible, and diverse retrosynthesis. In our model, as shown in Figure~\ref{fig:overview}, a forward reaction predictor and a retrosynthesis predictor are utilized to predict the chemically plausible synthetic routes of a target compound. To do this, after we find the optimal reactant sequence of retrosynthesis predictor $p(y|x)$, the result is fed into the forward reaction predictor, $p(\tilde{x}|y)$, to impose \textit{cycle consistency} to the optimal reactant, where $\tilde{x}$ denotes the forward reaction prediction outcome of the predicted reactant $y$. A reactant prediction, $y$, is considered to be cycle consistent when the predicted product, $\tilde{x}$, is equal to the original input product, $x$. We verify that the optimal reactant sequence, $y$, has cycle consistency as follows:
	\begin{equation}
	\argmax_{y\in\mathcal{Y}} p(\tilde{x}=x|y) \, p(y|x)
	\end{equation}
	
	To ensure diversity in the results, a multinomial latent variable, $z\in\{1,\cdots,K\}$, is introduced. The latent variable helps the model search multiple modes in the reaction space to generate diverse reactant candidates. For latent modeling, the likelihood above is modified as the sum of $K$ components as follows:
	\begin{equation}
	\begin{split}
	\label{eqn:latent_components}
	& \argmax_{y\in\mathcal{Y}} \sum_{z=1}^{K} p(\tilde{x}=x|z,y) \, p(z,y|x) \\
	& = \argmax_{y\in\mathcal{Y}} \sum_{z=1}^{K} p(\tilde{x}=x|z,y) \, p(y|z,x) \, p(z|x)
	\end{split}
	\end{equation}
	The retrosynthesis predictor, $p(y|z,x)$, and the forward reaction predictor, $p(\tilde{x}|z,y)$, are parametrized with the same transformer architecture. To control model complexity and regularize the model, the two transformers are strongly tied, i.e., they share most of the model parameters. This encourages the model to produce more valid and plausible predictions. The prior, $p(z|x)$, is parametrized with a learned multinomial distribution that is trained with the two transformers.

	In the next subsections, we will describe the architecture of the retrosynthesis and reaction prediction transformers as well as their parameter tying, latent modeling, and training/inference methods.
	
	
	\subsection{Retrosynthesis and Reaction Prediction Transformers}
	
	
	\begin{figure}
		\centering
		\includegraphics[width=0.8\linewidth]{./fig/tying.pdf}
		\caption{Architecture of the proposed model. Parameter tying is applied to the retrosynthesis transformer and the reaction prediction transformer.}
		\label{fig:tying}
	\end{figure}
	
	
	We use retrosynthesis and reaction prediction transformers, both of which share an architecture that was originally proposed in NMT \cite{vaswani2017attention} and has successfully been adapted to both retrosynthesis \cite{zheng2019predicting, chen2019learning,karpov2019transformer,lin2020automatic} and reaction prediction \cite{schwaller2019molecular} tasks. The two transformers have the same structure and share most modules, as depicted in Figure~\ref{fig:tying}. The retrosynthesis transformer maps the sequence of a target product's SMILES tokens to the sequence of reactant's SMILES tokens, while the reaction prediction transformer performs the task vice versa. The transformer architecture employs an encoder-decoder structure. The input sequence is encoded into a sequence of continuous representations and then decoded into the target sequence autoregressively. At each decoding step, the decoder takes the previously generated symbols as additional input and then generates the next element.
	
	The encoder and decoder components of the transformers consist of stacked layers. Each of the layers takes a sequence of vectors as input and generates a new sequence of vectors with the same shape. Encoder layers consist of two sub-layers: a multi-head self-attention sub-layer, followed by a position-wise feed-forward network (FFN) sub-layer. Decoder layers consist of three sub-layers: self-attention followed by encoder-decoder attention, followed by a position-wise FFN. Residual connection \cite{he2016deep} as well as layer normalization \cite{ba2016layer}, parametrized by mean and variance, are applied for every sub-layer individually. Masking is applied in the decoder self-attention to prevent a given output position from using information about future output positions. Prior to the first layer of the encoder and decoder, position encodings based on sinusoids of varying frequency are added to the word embedding to account for the order of the input sequence. To convert the decoder output to predicted next-token probabilities, a linear transformation, followed by softmax, is applied.
	
	A self-attention sub-layer has multiple attention heads in parallel, which query an entry $Q$ with key-value pairs $(K, V)$. For self-attention, $Q$, $K$, and $V$ are specified as hidden representations of the previous layer. For encoder-decoder attention, $K$ and $V$ become the final hidden representations of the encoder. The output is a weighted sum of the values $V$. To calculate the output from an attention head, the transformer uses a scaled dot-product, which is defined as 
	\begin{equation}
	\text{Attention}(Q, K, V) = \text{softmax}\big(\frac{QK^\top}{\sqrt{d}}\big) V,
	\end{equation}
	where $d$ is the dimensionality of the hidden representations. To form the sublayer output, results from each head are concatenated and a parameterized linear transformation is applied. A position-wise FFN sub-layer is a two-layer feedforward network with rectified linear unit (ReLU) activation, which is applied to each position separately and identically.
	
	
	\subsection{Parameter Tying}
	

	To control model complexity and regularize the models, we applied parameter sharing to the two transformers. Based on prior knowledge that the source and target languages are the same as the SMILES representation, previous studies shared vocabularies and word embedding of the encoder and decoder components \cite{schwaller2019molecular, zheng2019predicting,chen2019learning, karpov2019transformer,lin2020automatic}. We additionally tied the modules of the two transformers as depicted in Figure~\ref{fig:tying}. We bridged the two transformers by sharing components as follows: (i) the entire encoder, and (ii) the decoder except layer normalization. All attention and feed-forward modules are shared between two transformers whereas layer normalization are shared only within encoders. Compared to the single retrosynthesis transformer, the increase in the number of parameters in our model is negligible. Most parameters are shared across the two transformers, making the model compact, efficient, and highly generalizable \cite{xia2019tied}.
	
	
	\subsection{Latent Modeling}


	To generate diverse reactant candidates, we introduced a multinomial latent variable $z\in\{1,\cdots,K\}$ capturing multiple modes in the reaction space. As described in Eqn.~\ref{eqn:latent_components}, the likelihood is decomposed into $K$ components, where each component is a product of retrosynthetic prediction probability $p(y|z,x)$, forward reaction prediction probability $p(\tilde{x}=x|z,y)$, and prior $p(z|x)$. The retrosynthesis transformer generates the reactant SMILES tokens $y$ autoregressively given $z$ in addition to $x$. The reaction prediction transformer takes $z$ and $y$ as the input and calculates the likelihood of the product prediction $\tilde{x}$ being $x$. As depicted in Figure~\ref{fig:tying}, the decoder of the two transformers initialize the first token corresponding to $z$ (e.g., `<CLS2>') and then decode the tokens autoregressively. Prior $p(z|x)$ is calculated via a two-layer FFN with $tanh$ activation, which takes the mean of the encoded sequence of $x$ over time as the input. During the training phase, as the latent variable $z$ is unobservable, we use the online hard-EM algorithm. This is discussed in more detail in the next subsection.

	
	\subsection{Training}
	
	The training objective is minimizing the negative logarithm of the likelihood in Eqn.~\ref{eqn:latent_components}. To train the model with unobservable latent variable $z$, we utilized the online hard-EM algorithm. Then, the loss function becomes 
	\begin{equation}
	\mathcal{L}(\theta) = \mathbb{E}_{(x,y) \sim data} \big[ \min_{z} \mathcal{L}_h(x,y,z;\theta) \big],
	\end{equation}
	where $\mathcal{L}_h(x,y,z;\theta) = -\big( \log p(z|x;\theta) + \log p(y|z,x;\theta) + \log p(\tilde{x}=x|z,y;\theta) \big)$, and $\theta$ denotes the model parameters. To minimize the loss function for a given training set $\{(x^{(n)}, y^{(n)})\}_{n=1}^{N}$, we iteratively apply the following two steps: (i) \textit{E-step} estimating latent values $\{z^{(n)}\}_{n=1}^{N}$ that minimize loss $\mathcal{L}_h$ given current model parameters $\theta$ with 
	\begin{equation}
	z^{(n)} = \argmin_z \mathcal{L}_h (x^{(n)},y^{(n)},z;\theta)
	\end{equation}
	for all $n=1,\cdots,N$, and (ii) \textit{M-step} updating model parameters $\theta$ that minimize loss function given complete dataset $\{(x^{(n)}, y^{(n)}, z^{(n)})\}_{n=1}^{N}$ as 
	\begin{equation}
	\theta' = \argmin_\theta \sum_{n=1}^{N} \big[  \mathcal{L}_h(x^{(n)},y^{(n)},z^{(n)};\theta) \big].
	\end{equation}
	We used canonicalized datasets for training. During E-step, dropout is turned off to prevent the dropout noise corrupting the optimal value of $z$ \cite{chen2019learning}.
	
	\subsection{Inference}

 
	Given a target compound $x$, the model predicts the synthetic route of the compound in three steps, as depicted in Figure~\ref{fig:overview}: (1) infer a set of candidate reactants by applying retrosynthesis prediction transformer $p(y|z,x)$ with prior $p(z|x)$ for all $z\in\{1,\cdots,K\}$, (2) score the cycle consistency of the candidates using reaction prediction transformer $p(\tilde{x}=x|z,y)$, and (3) reorder the reactant candidates based on likelihood $p(z|x)\,p(y|z,x)\,p(\tilde{x}=x|z,y)$. In step (1), the decoder of the retrosynthesis transformer generates $K$ hypotheses by initializing the first token corresponding to $z$ (e.g., `<CLS2>') and then decoding the tokens autoregressively, which is straightforward and easily parallelizable. The reaction prediction transformer takes the canonicalized reactant prediction and the corresponding latent value.
	
	\subsection{Datasets}
	
	The effectiveness of the proposed method was demonstrated with two datasets: a popular benchmark reaction dataset derived from USPTO granted patents, specifically the USPTO-50K dataset containing approximately 50,000 reations, and a confidential in-house dataset.
	The former was curated by Liu et al. \cite{liu2017retrosynthetic} and classified into 10 reaction classes by Schneider et al. \cite{schneider2016s}. This dataset is randomly split into 40K, 5K, and 5K samples for training, validation, and test, and we used the same split. Although the reaction type information is included in this dataset, we did not use it, similar to the studies performed by Karpov et al. \cite{karpov2019transformer} and Chen et al. \cite{chen2019learning}, because in a real-world system, we cannot specify the exact reaction type of an anonymous product input in advance. The in-house dataset is curated by experienced chemists and contains approximately 162,000 reactions extracted from the Reaxys database \cite{reaxys}. Unlike the USPTO-50K dataset that contains only one reaction per product, the in-house dataset contains multiple reactions per product. This data set was randomly partitioned into 80\%, 10\%, and 10\% for training, validation, and testing, respectively, with no product overlap between partitions. In both datasets, a reaction is a pair that consists of the reactant and the target compound and is represented in the format of SMILES string. The reaction SMILES strings are tokenized into meaningful subunits of the molecules (e.g., atom or bond) in the same manner as in the work done by Schwaller et al. \cite{schwaller2019molecular}. All the molecules used in this work were canonicalized using RDKit \cite{landrum2006rdkit}.
	
	
	\subsection{Experimental Details}
	
	Our transformer is composed of 6 layers with 8 parallel attention heads. The sizes of word embedding, hidden nodes in the attention sub-layers, and the position-wise feed-forward network sub-layers are each set to 256. A relative position encoding of size 4 was utilized to consider representations of the relative positions or the distances between sequence elements \cite{shaw2018self}. We set the label smoothing parameter to 0 because a nonzero label smoothing parameter negatively affects its ability to discriminate between correct and incorrect predictions \cite{schwaller2019molecular}. We trained the model with the Adam optimizer \cite{kingma2014adam}, with hyper-parameter $\beta$ set to (0.9, 0.98). We set the initial learning rate as 2, with 8,000 warm-up steps. The gradients were accumulated over four batches and normalized by the number of tokens. We applied a dropout rate of 0.3. We used both validation loss and token-level accuracy as the evaluation measures of model performance. All experiments were conducted on 4 NVIDIA Tesla M40 GPUs, saving one checkpoint every 5,000 steps and averaging the last 5 checkpoints. For inference, we set beam size to be 10 so that the model produces 10 reactant candidates corresponding to each product used as input.
	
	
	\subsection{Baselines and Evaluation Metrics}
	
	For the USPTO-50K dataset, we compared the proposed model with seven template-free baselines from other studies \cite{liu2017retrosynthetic,karpov2019transformer,chen2019learning,lin2020automatic,zheng2019predicting} and three baselines implemented by us. The seven baselines are as follows. \textit{Liu-LSTM} is an RNN implemented by Liu et al.\cite{liu2017retrosynthetic}. \textit{Karpov-transformer} is a transformer trained with weights averaging and snapshot learning \cite{karpov2019transformer}. \textit{Chen-transformer} is a transformer with a multinomial latent variable that has a size of $K=1, 2, 5$ \cite{chen2019learning}; however, unlike us they set uniform prior for latent modeling. \textit{Lin-transformer} is a vanilla transformer implemented by Lin et al.\cite{lin2020automatic}. \textit{SCROP} is a model consisting of a retrosynthetic transformer and a syntax correction transformer. The syntax corrector fixes grammatical errors in the reactant output of the retrosynthetic transformer \cite{zheng2019predicting}. The three baselines we implemented for an ablation study are as follows: (i) base: a vanilla retrosynthesis transformer, (ii) base+CC: a retrosynthesis transformer and a reaction prediction transformer paired to check for cycle consistency (no parameter tying or latent modeling), and (iii) base+PT: a retrosynthesis transformer tied with a reaction prediction transformer and trained together (no cycle consistency check or latent modeling).
	

	As we set beam size to 10, the models take a product SMILES and output $10$ reactant SMILES candidates. Thus, for evaluation, we employed top-$k$ accuracy, top-$k$ invalid rate, and unique rate ($k=1,3,5,10$). The top-$k$ invalid rate is the proportion of SMILES predictions that shows a grammatical error among all predictions. Unique rate is the average of the ratio of unique molecules out of 10 predictions. In the case of the in-house dataset, which contains multiple ground truth reactions per target compound, we also used coverage to measure diversity in the candidates. For a given target product and its ground truth reactant set, coverage is defined as the average of the proportion of ground truth pathways correctly predicted by the model. This metric ranges between 0 and 1, and the high value means that the predicted candidates cover many pathways of the ground truth reaction set.


\section{Results and discussion}

	\subsection{Quantitative Evaluation Results on the Public Dataset}
	
	
	\begin{table*}
		\caption{\label{tab:uspto} Model performance on the USPTO-50K dataset (unit: \%).}
		\centering
		\small
		\begin{tabular}{@{\extracolsep{4pt}}lccccccccc@{}}
			\toprule
			\multirow{2}{*}{Model} & \multicolumn{4}{c}{Top-$k$ accuracy} & \multicolumn{4}{c}{Top-$k$ invalid rate} \\ 
			\cline{2-5} \cline{6-9}
			& \multicolumn{1}{c}{1} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} \\ 
			\midrule\midrule
			%		LSTM \cite{liu2017retrosynthetic} & 28.3 & 42.8 & 47.3 & 52.8 & 12.2 & 15.3 & 18.4 & 22.0  \\
			Liu-LSTM \cite{liu2017retrosynthetic} & 37.4 & 52.4 & 57.0 & 61.7 & 12.2 & 15.3 & 18.4 & 22.0  \\
			Karpov-transformer \cite{karpov2019transformer} & 40.6 & 42.7 & 63.9 & 69.8 & ~ & ~ & ~ & ~ \\
			Chen-transformer ($K=1$) \cite{chen2019learning} & 42.0 & 57.0 & 61.9 & 65.7 & ~ & ~ & ~ & ~  \\
			Chen-transformer ($K=2$) \cite{chen2019learning} & 42.1 & 60.0 & 64.9 & 70.3 & ~ & ~ & ~ & ~  \\
			%		Chen-transformer ($K=5$) \cite{chen2019learning} & 39.1 & \textbf{62.5} & \textbf{69.1} & \textbf{74.5} & ~ & ~ & ~ & ~ \\
			Chen-transformer ($K=5$) \cite{chen2019learning} & 39.1 & 62.5 & 69.1 & 74.5 & ~ & ~ & ~ & ~ \\
			Lin-transformer \cite{lin2020automatic} & 42.0 & \textbf{64.0} & \textbf{71.3} & \textbf{77.6} & 2.2 & 3.7 & 4.8 & 7.8 \\
			SCROP \cite{zheng2019predicting} & \textbf{43.7} & 60.0 & 65.2 & 68.7 & \textbf{0.7} & \textbf{1.4} & \textbf{1.8} & \textbf{2.3} \\
			\midrule
			base & 44.3 & 62.5 & 68.4 & 72.7 & 1.7 & 4.1 & 6.2 & 12.1\\ 
			base+CC & 45.3 & 64.5 & 70.1 & 72.7 & 0.2 & 0.4 & 1.2 & 12.1\\ 
			base+PT & 46.4 & 66.0 & 71.9 & 76.3 & 0.8 & 2.2 & 3.5 & 10.2\\ 
			proposed ($K=1$) & \textbf{47.1} & 67.1 & 73.1 & 76.3 & \textbf{0.1} & 0.2 & 0.6 & 10.2\\ 
			proposed ($K=2$) & 47.0 & \textbf{67.2} & 73.4 & 77.6 & \textbf{0.1} & 0.2 & 0.5 & 5.7\\ 
			proposed ($K=5$) & 46.8 & \textbf{67.2} & \textbf{73.5} & \textbf{78.5} & \textbf{0.1} & \textbf{0.1} & \textbf{0.3} & \textbf{2.6}\\ 
			\bottomrule
		\end{tabular}
	\end{table*}
	
	Table~\ref{tab:uspto} summarizes the top-$k$ accuracies and top-$k$ invalid rates evaluated with the USPTO-50K dataset. The best top-$k$ accuracies and invalid rates are shown in bold. Whereas the previous studies \cite{liu2017retrosynthetic,karpov2019transformer,chen2019learning,lin2020automatic,zheng2019predicting} reported the result scores evaluated on a single model, we trained five models with different seeds and averaged their scores. For latent modeling, we set $K=1, 2, 5$ as in Chen et al. \cite{chen2019learning}. 
	
	Overall, the proposed model outperformed the seven template-free baselines from the previous studies and the three baselines implemented by us for all $k=1,3,5,10$. The proposed ($K=5$) shows +3.1\%, +3.2\%, +2.2\%, and +0.9\% increases in the top-1, 3, 5, and 10 accuracies, respectively, compared to the best accuracies of the previous studies. Comparing the proposed ($K=5$) with the base transformer implemented by us, it shows +2.8\%, +4.7\%, +5.1\%, and +5.8\% increases in the top-1, 3, 5, and 10 accuracies, respectively. The top-$k$ invalid rate results reveal that only 0.1\% of the top-1 predictions are grammatically invalid, which is significantly better than those in previous publications and the base.
	
	To examine the effectiveness of the proposed cycle consistency check and parameter tying, we evaluated the base+CC and the base+PT models. The base+CC model is constructed by adding an additional reaction prediction transformer to the base model for a cycle consistency check. As no parameter tying is applied between the two transformers, the number of model parameters doubled from 17.4M to 34.8M. The top-1, 3, and 5 accuracies and invalid rates are improved, thereby indicating that the cycle consistency check process effectively reorders the candidates to increase the ranks of plausible reactions, including ground truths. Because the cycle consistency check just reorders the candidates, the top-10 accuracies and invalid rates remain the same. 
	
	The base+PT is constructed by adding an additional reaction prediction transformer to the base model and applying the proposed parameter tying method during training. During inference, however, it does not utilize the reaction prediction transformer and does not perform the cycle consistency check. Nevertheless, compared to the base and the base+CC models, this model shows improved top-$k$ accuracies and invalid rates with significant margin for all $k=1,3,5,10$. This is because, our parameter tying method bridges almost all parameters between the two transformers to the extent that the number of learnable parameters of the two transformers (17.5M) is approximately equal to that of a single retrosynthesis transformer (17.4M). Parameter sharing encourages one transformer to additionally view the data fed to another transformer and helps to learn from the richer data. Note that the proposed ($K=1$) is a model that utilizes the tied forward and backward transformers to conduct cycle consistency check. By reordering the candidates, the top-1, 3, and 5 accuracies and invalid rates are further improved.
	
	\begin{figure*}
	\centering
	\includegraphics[width=0.5\linewidth]{./fig/uniq.pdf}
	\caption{Unique rate evaluation on the USPTO-50K dataset (unit: \%).}
	\label{fig:uniq}
	\end{figure*}
	
	We also examine the effectiveness of latent modeling with a learned prior. As the number of latent classes $K$ increased from 1 to 5, the top-10 accuracy increased by +2.2\%, and the top-10 invalid rate decreased from 10.2\% to 2.6\%. While latent modeling is introduced to enhance diversity, we observed that it also leads to improvements in accuracy and grammatical error rates. To check whether latent modeling helps to increase the diversity, we evaluated the unique rate on the USPTO-50K dataset and depicted the results in Figure~\ref{fig:uniq}. As can be observed, the unique molecule rate grows as the number of latent classes $K$ increases, demonstrating that latent modeling is helpful for generating diverse reactant candidates. The diversity in candidates leads to the enhancement of the top-10 accuracy and invalid rate. Compared to the transformer from Chen et al. \cite{chen2019learning}, which applied multinomial latent modeling with uniform prior, both models demonstrate a trade-off between the top-1 and the top-10 accuracies as the number of latent classes $K$ increases. However, the degree of trade-off in our model is much smaller than the transformer of Chen et al. because of the difference in priors. As our model adopted a learned prior, it can choose the most appropriate latent class for a given molecule, whereas the model with uniform prior cannot. Thus, the degree of trade-off between the top-1 and the top-10 accuracies is much smaller for the learned prior than the uniform prior.
	

	\subsection{Quantitative Evaluation Results on the In-house Dataset}
	
	We also evaluated the proposed method on the in-house dataset. The top-$k$ accuracy, top-$k$ invalid rate, and unique rate results are summarized in Table~\ref{tab:inhouse}. Similar to the results from the public dataset, the proposed method outperformed the base model, a vanilla transformer, with significant margins, thereby demonstrating its ability to generate accurate and diverse predictions with fewer grammatical errors. Focusing on the significant improvement in the unique rate, we conducted an analysis to measure how many accurate and diverse candidates the model produced. The in-house dataset contains multiple reactions per product, unlike the USPTO-50K dataset, which contains only one reaction per product. For the analysis, 478 products, each with multiple ($\sim15$) reactions were selected from the in-house test set. The total number of reactions was 1250. Then, we evaluated coverage on this dataset. The coverage is defined as the average of the proportion of the correctly predicted ground truth reactions. Thus, high coverage means that the model is able to predict accurate and diverse reactant candidates. The results are summarized in Table~\ref{tab:coverage}. Among 478 products, 356 products have 2 distinct ground truth reactions, whereas 7 products have more than 10 ground truth reactions. Overall, the coverages of the proposed model are higher than those of the base. On average, the proposed model cover 87.3\% of the actual reaction pathways. This means that for a given target compound, the 10 candidates predicted by the proposed can cover 87.3\% of the actual reactions contained in the test set. Based on these results, it can be confirmed that the proposed model produces accurate and diverse candidates.	


\begin{table*}
	\caption{\label{tab:inhouse} Model performance on the in-house dataset (unit: \%).}
	\centering
	\small
	\begin{tabular}{@{\extracolsep{4pt}}lP{1cm}P{1cm}P{0.8cm}P{1cm}P{1cm}P{1cm}P{1cm}P{1cm}P{1.2cm}c@{}}
		\toprule
		\multirow{2}{*}{Model} & \multicolumn{4}{c}{Top-$k$ accuracy} & \multicolumn{4}{c}{Top-$k$ invalid rate} & Unique\\ 
		\cline{2-5} \cline{6-9}
		& \multicolumn{1}{c}{ 1 } & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{ 1 } & \multicolumn{1}{c}{ 3 } & \multicolumn{1}{c}{ 5 } & \multicolumn{1}{c}{10} & rate \\ 
		\midrule\midrule
		%		 base & 64.2 & 85.1 & 89.7 & 91.6 & 0.7 & 1.6 & 2.5 & 21.3\\ 
		%		
		base & 64.2 & 85.1 & 89.7 & 91.6 & 0.7 & 1.6 & 2.5 & 21.3 & 76.1\\ 
		proposed & \textbf{66.0} & \textbf{86.5} & \textbf{90.9} & \textbf{92.8} & \textbf{0.1} & \textbf{0.2} & \textbf{0.3} & \textbf{12.0}& \textbf{93.2}\\ 
		\bottomrule
	\end{tabular}
\end{table*}




\begin{table*}
	\caption{\label{tab:coverage} Coverage evaluation results on the in-house dataset (unit: \%).}
	\centering
	\small
	\begin{tabular}{@{\extracolsep{4pt}}P{2cm}P{2cm}P{2cm}P{2cm}@{}}
		\toprule
		\#reaction & \multirow{2}{*}{\#product} & \multicolumn{2}{c}{Coverage} \\ 
		\cline{3-4}
		per product & & base & proposed\\
		\midrule\midrule
		%1 & 10,623 & 89.3 & \textbf{91.2} \\
		2 & 356 & 85.8 & \textbf{88.9} \\
		3 & 68 & 83.3 & \textbf{86.3} \\
		4 & 15 & 90.0 & \textbf{91.7} \\
		5 & 20 & 78.0 & \textbf{80.0} \\
		6 & 5 & 76.7 & \textbf{80.0} \\
		7 & 4 & \textbf{60.7} & 57.1 \\
		8 & 1 & 75.0 & \textbf{87.5} \\
		9 & 2 & 66.7 & \textbf{77.8} \\
		10 & 0 & - & - \\
		11 & 2 & 63.6 & \textbf{72.7} \\
		12 & 1 & \textbf{50.0} & \textbf{50.0} \\
		13 & 1 & 38.5 & \textbf{46.2} \\
		14 & 2 & \textbf{50.0} & 42.9 \\
		15 & 1 & 46.7 & \textbf{53.3} \\
		\midrule
		\multicolumn{2}{c}{Average} & 84.4 & \textbf{87.3} \\
		\bottomrule
	\end{tabular}
\end{table*}


\subsection{Effectiveness of Cycle Consistency Check}	
	

To demonstrate the effectiveness of the cycle consistency check, we present an example of top-3 retrosynthetic predictions of a target compound before and after the check in Figure~\ref{fig:analysis}. The first column is the top-3 prediction results of the base model, whereas the second and third columns are the top-3 predictions before and after cycle consistency check, respectively. The base model produced grammatically invalid or chemically implausible paths, but the proposed model generated plausible paths. The retrosynthesis transformer of the proposed model successfully proposed the ground truth as the rank 2 prediction, and the cycle consistency check further increased the rank of the ground truth reactants to rank 1. Among chemically plausible predictions, one can utilize expense or productivity information to prioritize the best route.



\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{./fig/path.pdf}
	\caption{Example of retrosynthetic predictions before and after cycle consistency check for obtaining plausible candidates.}
	\label{fig:analysis}
\end{figure*}


	
\subsection{Effectiveness of Parameter Tying}
	

To investigate the effectiveness of parameter tying, we analyzed the attention weights calculated by the attention modules shared across the two transformers. The attention weights provide clues as to which input tokens were considered important when predicting a particular output sequence token. We depict the top-1 candidate’s attention maps of a reaction in Figure~\ref{fig:attention}. These maps are extracted from the base+CC and the proposed model. While both models have retrosynthetic and forward reaction transformers, the proposed parameter tying method is not applied to the base+CC. Although both models accurately predicted the correct synthetic pathway, the proposed model showed less noisy attention maps compared to the base. Moreover, the attention maps of the forward and backward predictions are consistent: the attention value patterns of the product-reactant token pairs are similar. In other words, the transpose of the retrosynthetic attention map is almost the same as the attention map of the forward reaction prediction. The resulting align between the input product tokens and the predicted reactant tokens is also reasonable. However, the attention maps of the base+CC are noisy and inconsistent. The attention map of the retrosynthetic prediction process is less reasonable than that of forward reaction prediction, thereby demonstrating that retrosynthesis prediction is more difficult than forward reaction prediction.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{./fig/attention2.pdf}
	\caption{Top-1 candidate's attention maps extracted from the retrosynthesis transformer and the reaction prediction transformer.}
	\label{fig:attention}
\end{figure}


	\subsection{Limitation}


	While the USPTO-50K dataset provides only one retrosynthetic pathway per target compound, there can be multiple. For example, in Figure~\ref{fig:analysis}, it is shown that there are chemically plausible reactions that are not included in the dataset. In practice, chemists select the best route by considering factors such as complexity, expense, productivity, and safety \cite{zheng2019predicting}. Thus, for complete synthesis planning, reaction condition prediction (e.g., solvent, catalyst, and temperature), chemical reactivity prediction, and expense analysis should all be incorporated with the retrosynthesis prediction. For a complex target compound, multi-step retrosynthesis is required to decompose the target recursively. A possible option is to combining a Monte Carlo tree search algorithm with a single-step retrosynthesis model \cite{lin2020automatic}.
	
	Another limitation is the data representation. We used SMILES representation, which is obtained by traversing the chemical graph of a molecule in a depth-first way. It linearizes molecules as strings and does not utilize the inherently rich chemical structure. Thus, incorporating molecular graph information, in addition to SMILES \cite{maziarka2020molecule} or graph-to-graph modeling \cite{shi2020graph}, will further enhance the retrosynthesis accuracy.

	
	\section{Conclusion}
	
	In this paper, we proposed tied two-way transformers for enhancing validity, plausibility, and diversity in retrosynthesis. This was enabled by introducing cycle consistency checks, parameter tying, and latent modeling. Throughout the experimental results evaluated on public and in-house datasets, we found that the proposed model outputs more accurate, valid, and diverse reactants compared to the state-of-the-art baselines. Our method achieved the top-1 accuracy of 47.1\% and the top-10 accuracy of 78.5\% on the USPTO-50K dataset without a priori reaction type information. The top-1 predictions have very few grammatical errors, with an invalid rate of 0.1\% on both USPTO-50K and in-house datasets. Latent modeling increased the unique molecule rate to 91.4\%. To investigate the effectiveness of cycle consistency checks, parameter tying, and latent modeling, we also conducted pathway analysis, attention analysis, and coverage analysis. We expect that this method can lower the burden of chemists in synthesis planning by providing more informative and novel retrosynthesis results.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% The "Acknowledgement" section can be given in all manuscript
	%% classes.  This should be given within the "acknowledgement"
	%% environment, which will make the correct section or running title.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\begin{acknowledgement}
	%
	%The authors thank the anonymous reviewers for their valuable comments.
	
	%\end{acknowledgement}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% The same is true for Supporting Information, which should use the
	%% suppinfo environment.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\begin{suppinfo}
	%
	%A listing of the contents of each file supplied as Supporting Information
	%should be included. For instructions on what should be included in the
	%Supporting Information as well as how to prepare this material for
	%publications, refer to the journal's Instructions for Authors.
	%
	%The following files are available free of charge.
	%\begin{itemize}
	%  \item Filename: brief description
	%  \item Filename: brief description
	%\end{itemize}
	%
	%\end{suppinfo}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% The appropriate \bibliography command should be placed here.
	%% Notice that the class file automatically sets \bibliographystyle
	%% and also names the section correctly.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\bibliography{achemso-demo}
	
	
\end{document}
